{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import bm25s\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bm25s_scores(query: str, df: pd.DataFrame):\n",
    "    corpus = (df[\"title\"].astype(str) + \": \" + df[\"abstract\"].astype(str)).tolist()\n",
    "    corpus_tokens = bm25s.tokenize(corpus, stopwords=\"english\")\n",
    "\n",
    "    retriever = bm25s.BM25(method=\"lucene\")\n",
    "    retriever.index(corpus_tokens)\n",
    "    \n",
    "    # Tokenize query\n",
    "    query_tokens = bm25s.tokenize(query, stopwords=\"english\")\n",
    "    \n",
    "    # Get scores for all documents\n",
    "    # retrieve returns (docs, scores) but we only need scores\n",
    "    _, scores = retriever.retrieve(query_tokens, k=len(corpus))\n",
    "    \n",
    "    # BM25s returns scores in shape (1, k) - flatten to 1D\n",
    "    scores = scores.flatten()\n",
    "    \n",
    "    # Normalize scores to 0-1 range\n",
    "    if scores.max() > 0:\n",
    "        scores_normalized = scores / scores.max()\n",
    "    else:\n",
    "        scores_normalized = np.zeros(len(corpus))\n",
    "    return rankdata(-scores_normalized, \"dense\")\n",
    "\n",
    "########################################################\n",
    "\n",
    "def tfidf_vectorizer(query: str, papers: list[str]):\n",
    "    if not papers:\n",
    "        return np.array([])\n",
    "    corpus = [query] + papers\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n",
    "\n",
    "    # Normalize scores to 0-1 range\n",
    "    scores_normalized = similarities / (similarities.max() or 1)\n",
    "    return rankdata(-scores_normalized, \"dense\")\n",
    "\n",
    "\n",
    "#####################################################\n",
    "\n",
    "def compute_lsa_scores(query: str, df: pd.DataFrame, n_components: int = 100):\n",
    "    \"\"\"\n",
    "    Computes LSA scores for a query against a dataframe of documents.\n",
    "\n",
    "    NOTE: This is the 'inefficient' way, as it re-builds the entire\n",
    "    LSA model for every single query. See the class-based\n",
    "    example below for a much more efficient, 'correct' implementation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Create the corpus\n",
    "    corpus = (df[\"title\"].astype(str) + \": \" + df[\"abstract\"].astype(str)).tolist()\n",
    "    \n",
    "    # --- LSA Indexing ---\n",
    "    # 2. Create the TF-IDF matrix (This is the input to LSA)\n",
    "    # We filter out very common and very rare words, which helps LSA\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_df=0.8, min_df=5)\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    # 3. Apply Truncated SVD (This *is* LSA)\n",
    "    # This finds the 'topics' or 'concepts' in the text\n",
    "    svd_model = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "    \n",
    "    # 4. Create the LSA document matrix (documents transformed into 'topic space')\n",
    "    lsa_matrix = svd_model.fit_transform(tfidf_matrix)\n",
    "    \n",
    "    # --- LSA Querying ---\n",
    "    # 5. Transform the query into the same LSA 'topic space'\n",
    "    # 5a. First, get the query's TF-IDF vector\n",
    "    query_tfidf = vectorizer.transform([query])\n",
    "    # 5b. Then, transform that vector using the SVD model\n",
    "    query_lsa = svd_model.transform(query_tfidf)\n",
    "    \n",
    "    # 6. Calculate Similarity\n",
    "    # We use cosine similarity to find documents that are 'close' to\n",
    "    # the query in the LSA topic space.\n",
    "    scores = cosine_similarity(query_lsa, lsa_matrix)\n",
    "    \n",
    "    # 7. Flatten and Rank\n",
    "    scores = scores.flatten()\n",
    "    return rankdata(-scores, \"dense\")\n",
    "\n",
    "\n",
    "#################################################################\n",
    "\n",
    "def compute_bm25_centroid_ranks(df: pd.DataFrame, centroid_k: int = 5):\n",
    "    \"\"\"\n",
    "    Ranks all papers by similarity to the centroid of the\n",
    "    top 'centroid_k' BM25-ranked papers.\n",
    "    \n",
    "    This function is designed to run *after* a 'bm25_rank'\n",
    "    column already exists on the DataFrame.\n",
    "    \n",
    "    It correctly handles missing embeddings by assigning them\n",
    "    the worst possible rank.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Step 1: Clean the Data ---\n",
    "    \n",
    "    # 1a. Extract vectors, safely handling empty dicts\n",
    "    df['specter_vector'] = df['embedding'].apply(lambda x: x.get('specter_v2'))\n",
    "    \n",
    "    # 1b. Create clean df, but *keep the original index*\n",
    "    # This index is crucial for mapping back to the full df.\n",
    "    df_clean = df.dropna(subset=['specter_vector']).copy()\n",
    "    \n",
    "    # 1c. Create the clean 2D NumPy array\n",
    "    try:\n",
    "        embeddings_clean_all = np.stack(df_clean['specter_vector'].values)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: Could not stack embeddings. Are they all the same length? {e}\")\n",
    "        # Return a useless rank for all rows if it fails\n",
    "        return np.full(len(df), len(df), dtype=int)\n",
    "        \n",
    "    # --- Step 2: Create Pseudo-Query (Centroid) ---\n",
    "    \n",
    "    # 2a. Find top 'k' papers *from df_clean* based on bm25_rank\n",
    "    # *** FIX: Use ascending=True because rank 1 is the best ***\n",
    "    top_k_papers = df_clean.sort_values(by=\"bm25_rank\", ascending=True).head(centroid_k)\n",
    "    \n",
    "    # 2b. Get the integer row positions (ilocs) of these papers\n",
    "    # within the 'df_clean' dataframe.\n",
    "    top_k_ilocs = df_clean.index.get_indexer(top_k_papers.index)\n",
    "\n",
    "    # 2c. Get vectors using these ilocs from the clean array\n",
    "    top_k_vectors = embeddings_clean_all[top_k_ilocs]\n",
    "    \n",
    "    # 2d. Create the centroid\n",
    "    centroid_query_vector = np.mean(top_k_vectors, axis=0).reshape(1, -1)\n",
    "    \n",
    "    # --- Step 3: Re-rank ---\n",
    "    \n",
    "    # 3a. Calculate similarity against all 140 clean embeddings\n",
    "    scores = cosine_similarity(centroid_query_vector, embeddings_clean_all)\n",
    "    scores = scores.flatten() # Shape (140,)\n",
    "    \n",
    "    # --- Step 4: Map Ranks Back to Original DF (Fixes the ValueError) ---\n",
    "    \n",
    "    # 4a. Create a Series of scores, using df_clean's index\n",
    "    # This aligns the 140 scores with their original 166-based indices\n",
    "    ranked_scores_series = pd.Series(scores, index=df_clean.index)\n",
    "    \n",
    "    # 4b. Reindex to the *full* original df's index (166 rows)\n",
    "    # This fills the 26 missing rows (with no embeddings) with NaN\n",
    "    full_scores_series = ranked_scores_series.reindex(df.index)\n",
    "    \n",
    "    # 4c. Fill NaN with a very bad score (-1, since similarity is 0-1)\n",
    "    # These papers couldn't be scored, so they are the least similar.\n",
    "    full_scores_series = full_scores_series.fillna(-np.inf) \n",
    "    \n",
    "    # 4d. Now, rank this *full* 166-length array\n",
    "    # The -1 scores will correctly be given the worst (highest) rank.\n",
    "    return rankdata(-full_scores_series.values, \"dense\")\n",
    "\n",
    "#################################################################\n",
    "\n",
    "def compute_rrf_relevance(df):\n",
    "    return (\n",
    "        1 / (df['bm25_rank'] + 60) +\n",
    "        1 / (df['tfid_rank'] + 60) +\n",
    "        1 / (df['lsa_rank'] + 60) +\n",
    "        1 / (df['pseudo_specter_rank'] + 60)\n",
    "    )\n",
    "\n",
    "\n",
    "##################################################################\n",
    "\n",
    "def compute_authority_scores(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Computes authority scores based on influential citations.\n",
    "    Returns dense ranks (1 = best).\n",
    "    \"\"\"\n",
    "    # Use influentialCitationCount, fallback to citationCount * 0.3\n",
    "    df['authority_raw'] = df.apply(\n",
    "        lambda row: np.log10(\n",
    "            max(row.get('influentialCitationCount', 0), \n",
    "                row.get('citationCount', 0) * 0.3) + 1\n",
    "        ), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Rank: higher authority = lower rank number\n",
    "    return rankdata(-df['authority_raw'].values, method='dense')\n",
    "\n",
    "\n",
    "################################################################\n",
    "\n",
    "def compute_recency_scores(df: pd.DataFrame, current_year: int = 2025, lambda_decay: float = 0.1):\n",
    "    \"\"\"\n",
    "    Computes recency scores using exponential decay.\n",
    "    Returns dense ranks (1 = newest/best).\n",
    "    \n",
    "    lambda_decay controls half-life:\n",
    "    - 0.1 = ~10 year half-life (general science)\n",
    "    - 0.3 = ~3 year half-life (fast-moving fields like ML)\n",
    "    \"\"\"\n",
    "    df['recency_raw'] = df['year'].apply(\n",
    "        lambda year: np.exp(-lambda_decay * (current_year - year)) if pd.notna(year) else 0\n",
    "    )\n",
    "    \n",
    "    # Rank: higher recency score = lower rank number\n",
    "    return rankdata(-df['recency_raw'].values, method='dense')\n",
    "\n",
    "#######################################################################\n",
    "\n",
    "def normalize_scores(df: pd.DataFrame, score_columns: list):\n",
    "    \"\"\"\n",
    "    Applies min-max normalization to score columns.\n",
    "    Handles edge case where max == min.\n",
    "    \"\"\"\n",
    "    for col in score_columns:\n",
    "        min_val = df[col].min()\n",
    "        max_val = df[col].max()\n",
    "        \n",
    "        if max_val - min_val > 0:\n",
    "            df[f'{col}_norm'] = (df[col] - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            # All scores are identical - set to 0.5 (neutral)\n",
    "            df[f'{col}_norm'] = 0.5\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_final_ranking(df: pd.DataFrame, \n",
    "                          w_relevance: float = 0.60,\n",
    "                          w_authority: float = 0.35,\n",
    "                          w_recency: float = 0.05):\n",
    "    \"\"\"\n",
    "    Computes final weighted score from all normalized signals.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- THIS IS THE FIX ---\n",
    "    # 1. Compute the raw RRF SCORE (higher is better)\n",
    "    df['rrf_relevance_SCORE'] = compute_rrf_relevance(df)\n",
    "    \n",
    "    # 2. Convert the SCORE to a RANK (lower is better)\n",
    "    #    We rank the *negative* score, so the highest score gets rank 1.\n",
    "    df['rrf_relevance_rank'] = rankdata(-df['rrf_relevance_SCORE'].values, method='dense')\n",
    "    # --- END FIX ---\n",
    "\n",
    "    # Compute other raw ranks (lower rank = better)\n",
    "    df['authority_rank'] = compute_authority_scores(df)\n",
    "    df['recency_rank'] = compute_recency_scores(df)\n",
    "    \n",
    "    # CRITICAL: Normalize ranks directly using min-max\n",
    "    df = normalize_scores(df, ['rrf_relevance_rank', 'authority_rank', 'recency_rank'])\n",
    "    \n",
    "    # Invert normalized ranks so higher = better (this logic is correct)\n",
    "    df['relevance_score'] = 1.0 - df['rrf_relevance_rank_norm']\n",
    "    df['authority_score'] = 1.0 - df['authority_rank_norm']\n",
    "    df['recency_score'] = 1.0 - df['recency_rank_norm']\n",
    "    \n",
    "    # Final weighted combination\n",
    "    df['final_score'] = (\n",
    "        w_relevance * df['relevance_score'] +\n",
    "        w_authority * df['authority_score'] +\n",
    "        w_recency * df['recency_score']\n",
    "    )\n",
    "    \n",
    "    return df.sort_values('final_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =  \"adverse mental health outcomes social media use adolescents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path.cwd().parents[0].joinpath(\"searches\").glob(\"*.json\")\n",
    "df = pd.concat([pd.read_json(i) for i in file_path]).reset_index(drop = True)\n",
    "    # .drop(columns = [\"paperId\", \"openAccessPdf\", \"authors\", \"venue\"])\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(tfid_rank = lambda df: tfidf_vectorizer(query, df.title.to_list()),\n",
    "          bm25_rank = lambda df: compute_bm25s_scores(query, df),\n",
    "          lsa_rank = lambda df: compute_lsa_scores(query, df),\n",
    "          pseudo_specter_rank = lambda df: compute_bm25_centroid_ranks(df),\n",
    "          rrf_score = lambda df: compute_rrf_relevance(df),\n",
    "          authority_rank = lambda df: compute_authority_scores(df),\n",
    "          recency_rank = lambda df: compute_recency_scores(df, 2025)\n",
    "         )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_final_ranking(df)\n",
    "    # .openAccessPdf.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
